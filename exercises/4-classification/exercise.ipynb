{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification on `emnist`\n",
    "\n",
    "## 1. Create `Readme.md` to document your work\n",
    "\n",
    "Explain your choices, process, and outcomes.\n",
    "\n",
    "## 2. Classify all symbols\n",
    "\n",
    "### Choose a model\n",
    "\n",
    "Your choice of model! Choose wisely...\n",
    "\n",
    "### Train away!\n",
    "\n",
    "Is do you need to tune any parameters? Is the model expecting data in a different format?\n",
    "\n",
    "### Evaluate the model\n",
    "\n",
    "Evaluate the models on the test set, analyze the confusion matrix to see where the model performs well and where it struggles.\n",
    "\n",
    "### Investigate subsets\n",
    "\n",
    "On which classes does the model perform well? Poorly? Evaluate again, excluding easily confused symbols (such as 'O' and '0').\n",
    "\n",
    "### Improve performance\n",
    "\n",
    "Brainstorm for improving the performance. This could include trying different architectures, adding more layers, changing the loss function, or using data augmentation techniques.\n",
    "\n",
    "## 2. Classify digits vs. letters model showdown\n",
    "\n",
    "Perform a full showdown classifying digits vs letters:\n",
    "\n",
    "1. Create a column for whether each row is a digit or a letter\n",
    "2. Choose an evaluation metric \n",
    "3. Choose several candidate models to train\n",
    "4. Divide data to reserve a validation set that will NOT be used in training/testing\n",
    "5. K-fold train/test\n",
    "    1. Create train/test splits from the non-validation dataset \n",
    "    2. Train each candidate model (best practice: use the same split for all models)\n",
    "    3. Apply the model the the test split \n",
    "    4. (*Optional*) Perform hyper-parametric search\n",
    "    5. Record the model evaluation metrics\n",
    "    6. Repeat with a new train/test split\n",
    "6. Promote winner, apply model to validation set\n",
    "7. (*Optional*) Perform hyper-parametric search, if applicable\n",
    "8. Report model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (once per virtual environment)\n",
    "# %pip install numpy pandas matplotlib seaborn scikit-learn xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import os\n",
    "import string\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import emnist\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Model evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "# ML packages\n",
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# XGBoost (SVM)\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Constants\n",
    "SIZE = 28\n",
    "REBUILD = True\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # or 3 to suppress all warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions\n",
    "def int_to_char(label):\n",
    "    \"\"\"Convert an integer label to the corresponding uppercase character.\"\"\"\n",
    "    if label < 10:\n",
    "        return str(label)\n",
    "    elif label < 36:\n",
    "        return chr(label - 10 + ord('A'))\n",
    "    else:\n",
    "        return chr(label - 36 + ord('a'))\n",
    "\n",
    "def show_image(row):\n",
    "    \"\"\"Display a single image and its corresponding label.\"\"\"\n",
    "    image = row['image']\n",
    "    label = row['label']\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.title('Label: ' + int_to_char(label))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def show_grid(data, title=None, num_cols=5, figsize=(20, 10)):\n",
    "    \"\"\"\n",
    "    Display a list of images as a grid of num_cols columns.\n",
    "    images: a list of images, each represented as a 28x28 numpy array\n",
    "    labels: a list of labels, one for each image\n",
    "    title: (optional) a title for the plot\n",
    "    num_cols: (optional) number of columns to use in the grid\n",
    "    figsize: (optional) size of the figure\n",
    "    \"\"\"\n",
    "    num_images = len(data)\n",
    "    num_rows = (num_images - 1) // num_cols + 1\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=figsize)\n",
    "    if title is not None:\n",
    "        fig.suptitle(title, fontsize=16)\n",
    "    for i in range(num_rows):\n",
    "        for j in range(num_cols):\n",
    "            index = i * num_cols + j\n",
    "            if index < num_images:\n",
    "                axes[i, j].imshow(data.iloc[index]['image'], cmap='gray')\n",
    "                axes[i, j].axis('off')\n",
    "                label = int_to_char(data.iloc[index]['label'])\n",
    "                axes[i, j].set_title(label)\n",
    "    plt.show()\n",
    "\n",
    "# Get a random image of a given label from the dataset\n",
    "def get_image_by_label(data, label):\n",
    "    \"\"\"Get a random image of a given label from the dataset.\"\"\"\n",
    "    images = data[data['label'] == label]['image'].tolist()\n",
    "    return random.choice(images)\n",
    "\n",
    "# Plot the training and validation accuracy during the training of a model\n",
    "def plot_accuracy(history):\n",
    "    \"\"\"Plot the training and validation accuracy during the training of a model.\"\"\"\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "    plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
    "    plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the training and validation loss during the training of a model\n",
    "def plot_loss(history):\n",
    "    \"\"\"Plot the training and validation loss during the training of a model.\"\"\"\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Normalize the pixel values of the images in the dataset to have zero mean and unit variance\n",
    "# This is a common preprocessing step for neural networks, but may not be necessary in all cases\n",
    "def normalize_images(images):\n",
    "    \"\"\"Normalize the pixel values of the images in the dataset to have zero mean and unit variance.\"\"\"\n",
    "    images = np.array(images)\n",
    "    mean = images.mean()\n",
    "    std = images.std()\n",
    "    images = (images - mean) / std\n",
    "    return images.tolist()\n",
    "\n",
    "# Display metrics for a model\n",
    "def display_metrics(task, model_name, metrics_dict):\n",
    "    \"\"\"Display performance metrics and confusion matrix for a model.\"\"\"\n",
    "    metrics_df = pd.DataFrame()\n",
    "    cm_df = pd.DataFrame()\n",
    "    for key, value in metrics_dict[task][model_name].items():\n",
    "            metrics_df[key] = [value]\n",
    "    display(Markdown(f'# Performance Metrics: {model_name}'))\n",
    "    display(metrics_df)\n",
    "\n",
    "\n",
    "def display_metrics_cm(task, model_name, metrics_dict):\n",
    "    \"\"\"Display performance metrics and confusion matrix for a model.\"\"\"\n",
    "    metrics_df = pd.DataFrame()\n",
    "    cm_df = pd.DataFrame()\n",
    "    for key, value in metrics_dict[task][model_name].items():\n",
    "        if type(value) == np.ndarray:\n",
    "            cm_df = pd.DataFrame(value, index=['actual int', 'actual letter'], columns=['predicted int', 'predicted letter'])\n",
    "        else:\n",
    "            metrics_df[key] = [value]\n",
    "    display(Markdown(f'# Performance Metrics: {model_name}'))\n",
    "    display(metrics_df)\n",
    "    display(Markdown(f'# Confusion Matrix: {model_name}'))\n",
    "    display(cm_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "# Extract the training split as images and labels\n",
    "image, label = emnist.extract_training_samples('byclass')\n",
    "\n",
    "# Add columns for each pixel value (28x28 = 784 columns)\n",
    "train = pd.DataFrame()\n",
    "\n",
    "# Add a column with the image data as a 28x28 array\n",
    "train['image'] = list(image)\n",
    "train['image_flat'] = train['image'].apply(lambda x: np.array(x).reshape(-1))\n",
    "\n",
    "# Add a column showing the label\n",
    "train['label'] = label\n",
    "\n",
    "# Convert labels to characters\n",
    "class_label = np.array([int_to_char(l) for l in label])\n",
    "\n",
    "# Add a column with the character corresponding to the label\n",
    "train['class'] = class_label\n",
    "\n",
    "# Repeat for the test split\n",
    "image, label = emnist.extract_test_samples('byclass')\n",
    "class_label = np.array([int_to_char(l) for l in label])\n",
    "valid = pd.DataFrame()\n",
    "valid['image'] = list(image)\n",
    "valid['image_flat'] = valid['image'].apply(lambda x: np.array(x).reshape(-1))\n",
    "valid['label'] = label\n",
    "valid['class'] = class_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All symbols classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For a preliminary analysis\n",
    "train_subset = train.sample(n= 5000, replace=False, random_state=42)\n",
    "valid_subset = valid.sample(n= 2000, replace=False, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If use all data\n",
    "# train_subset = train\n",
    "# valid_subset = valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the predictors and target\n",
    "X = train_subset.drop('label',axis=1)\n",
    "y = train_subset.label\n",
    "\n",
    "# Stratified sampling divides the data set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Check the size of each dataset\n",
    "print(\"Training set size:\", X_train.shape[0])\n",
    "print(\"Test set size:\", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All symbols classifier: XGBoost\n",
    "task = 'All_symbols'\n",
    "model_name = 'xgboost'\n",
    "\n",
    "# Initializa XGBoost classifier\n",
    "xgb_clf = XGBClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train and evaluate the model\n",
    "xgb_clf.fit(X_train['image_flat'].tolist(),y_train)\n",
    "y_pred = xgb_clf.predict(X_test['image_flat'].tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate performance metrics\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred,average='weighted',zero_division=0)\n",
    "rec = recall_score(y_test, y_pred,average='weighted',zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred,average='weighted',zero_division=0)\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict = {\n",
    "    'All_symbols': {\n",
    "        'xgboost': {\n",
    "            'accuracy': [],\n",
    "            'precision': [],\n",
    "            'recall': [],\n",
    "            'f1': []\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the results to metrics_dict\n",
    "metrics_dict[task][model_name] = {'accuracy': acc,\n",
    "                                  'precision': prec,\n",
    "                                  'recall': rec,\n",
    "                                  'f1': f1,\n",
    "                                  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display performance metrics\n",
    "display_metrics(task, model_name, metrics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the performance\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "\n",
    "# create a new figure\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# set the x-axis labels\n",
    "labels = ['accuracy', 'precision', 'recall', 'f1']\n",
    "\n",
    "# iterate over the models in the metrics_dict\n",
    "for i, model in enumerate(metrics_dict['All_symbols'].keys()):\n",
    "    # extract the metric values for this model\n",
    "    values = [metrics_dict['All_symbols'][model][label] for label in labels]\n",
    "    \n",
    "    # plot the values as a line\n",
    "    ax.plot(labels, values, label=model, marker='o', color=colors[i])\n",
    "\n",
    "    \n",
    "# add a legend and title\n",
    "ax.legend()\n",
    "ax.set_title('Performance Metrics')\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate subsets\n",
    "# Make subsets\n",
    "## O and 0\n",
    "symbols_list = ['O', '0']\n",
    "mask_valid = X_test['class'].apply(lambda x: x in symbols_list)\n",
    "valid_01_X = X_test[mask_valid]\n",
    "valid_01_y = y_test[mask_valid]\n",
    "valid_01_X.reset_index(drop=True, inplace=True)\n",
    "valid_01_y.reset_index(drop=True, inplace=True)\n",
    "## l and 1\n",
    "symbols_list = ['l', '1']\n",
    "mask_valid = X_test['class'].apply(lambda x: x in symbols_list)\n",
    "valid_02_X = X_test[mask_valid]\n",
    "valid_02_y = y_test[mask_valid]\n",
    "valid_02_X.reset_index(drop=True, inplace=True)\n",
    "valid_02_y.reset_index(drop=True, inplace=True)\n",
    "## Z and 2\n",
    "symbols_list = ['Z', '2']\n",
    "mask_valid = X_test['class'].apply(lambda x: x in symbols_list)\n",
    "valid_03_X = X_test[mask_valid]\n",
    "valid_03_y = y_test[mask_valid]\n",
    "valid_03_X.reset_index(drop=True, inplace=True)\n",
    "valid_03_y.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict_subsets = {\n",
    "    'Investigate_subsets_using_xgboost' : { # task name (Investigate_subsets_using_xgboost)\n",
    "        'O vs 0': {\n",
    "            'accuracy': [],\n",
    "            'precision': [],\n",
    "            'recall': [],\n",
    "            'f1': [],\n",
    "            'confusion matrix': []\n",
    "        },\n",
    "        'l vs 1': {\n",
    "            'accuracy': [],\n",
    "            'precision': [],\n",
    "            'recall': [],\n",
    "            'f1': [],\n",
    "            'confusion matrix': []\n",
    "        },\n",
    "        'Z vs 2': {\n",
    "            'accuracy': [],\n",
    "            'precision': [],\n",
    "            'recall': [],\n",
    "            'f1': [],\n",
    "            'confusion matrix': []\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict_subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All symbols classifier: XGBoost\n",
    "task = 'Investigate_subsets_using_xgboost'\n",
    "\n",
    "y_pred_01 = xgb_clf.predict(valid_01_X['image_flat'].tolist())\n",
    "y_pred_02 = xgb_clf.predict(valid_02_X['image_flat'].tolist())\n",
    "y_pred_03 = xgb_clf.predict(valid_03_X['image_flat'].tolist())\n",
    "\n",
    "# O vs 0\n",
    "## Calculate performance metrics\n",
    "model_name = 'O vs 0'\n",
    "acc = accuracy_score(valid_01_y, y_pred_01)\n",
    "prec = precision_score(valid_01_y, y_pred_01,average='weighted',zero_division=0)\n",
    "rec = recall_score(valid_01_y, y_pred_01,average='weighted',zero_division=0)\n",
    "f1 = f1_score(valid_01_y, y_pred_01,average='weighted',zero_division=0)\n",
    "\n",
    "unique = valid_01_y.unique()[1]\n",
    "y_pred_01_replaced = np.where(y_pred_01!=unique,0,y_pred_01)\n",
    "cm1 = confusion_matrix(valid_01_y, y_pred_01_replaced)\n",
    "\n",
    "## Store performance metrics in dictionary\n",
    "metrics_dict_subsets[task][model_name] = {'accuracy' : acc,\n",
    "                                 'precision' : prec,\n",
    "                                 'recall' : rec,\n",
    "                                 'f1' : f1,\n",
    "                                 'confusion matrix': cm1\n",
    "                                 }\n",
    "# Display performance metrics\n",
    "display_metrics_cm(task, model_name, metrics_dict_subsets)\n",
    "\n",
    "# l vs 1\n",
    "## Calculate performance metrics\n",
    "model_name = 'l vs 1'\n",
    "acc = accuracy_score(valid_02_y, y_pred_02)\n",
    "prec = precision_score(valid_02_y, y_pred_02,average='weighted',zero_division=0)\n",
    "rec = recall_score(valid_02_y, y_pred_02,average='weighted',zero_division=0)\n",
    "f1 = f1_score(valid_02_y, y_pred_02,average='weighted',zero_division=0)\n",
    "\n",
    "unique = valid_02_y.unique()[1]\n",
    "y_pred_02_replaced = np.where(y_pred_02!=unique,1,y_pred_02)\n",
    "cm2 = confusion_matrix(valid_02_y, y_pred_02_replaced)\n",
    "\n",
    "## Store performance metrics in dictionary\n",
    "metrics_dict_subsets[task][model_name] = {'accuracy' : acc,\n",
    "                                 'precision' : prec,\n",
    "                                 'recall' : rec,\n",
    "                                 'f1' : f1,\n",
    "                                 'confusion matrix': cm2\n",
    "                                 }\n",
    "# Display performance metrics\n",
    "display_metrics_cm(task, model_name, metrics_dict_subsets)\n",
    "\n",
    "# Z vs 2\n",
    "## Calculate performance metrics\n",
    "model_name = 'Z vs 2'\n",
    "acc = accuracy_score(valid_03_y, y_pred_03)\n",
    "prec = precision_score(valid_03_y, y_pred_03,average='weighted',zero_division=0)\n",
    "rec = recall_score(valid_03_y, y_pred_03,average='weighted',zero_division=0)\n",
    "f1 = f1_score(valid_03_y, y_pred_03,average='weighted',zero_division=0)\n",
    "\n",
    "unique = valid_03_y.unique()[1]\n",
    "y_pred_03_replaced = np.where(y_pred_03!=unique,2,y_pred_03)\n",
    "cm3 = confusion_matrix(valid_03_y, y_pred_03_replaced)\n",
    "\n",
    "\n",
    "## Store performance metrics in dictionary\n",
    "metrics_dict_subsets[task][model_name] = {'accuracy' : acc,\n",
    "                                 'precision' : prec,\n",
    "                                 'recall' : rec,\n",
    "                                 'f1' : f1,\n",
    "                                 'confusion matrix': cm3\n",
    "                                 }\n",
    "\n",
    "# Display performance metrics\n",
    "display_metrics_cm(task, model_name, metrics_dict_subsets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'min_child_weight': [1, 2, 3],\n",
    "    'subsample': [0.8, 0.9, 1.0]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " X_test_para = X_test['image_flat'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator=xgb_clf, param_grid=param_grid, cv=3, scoring='f1', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_test['image_flat'].tolist(), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best Score: {best_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winning_mdl = XGBClassifier(**grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winning_mdl.fit(X_test['image_flat'].tolist(), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = winning_mdl.predict(valid_subset['image_flat'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(valid_subset.label, y_pred)\n",
    "prec = precision_score(valid_subset.label, y_pred,average='weighted',zero_division=0)\n",
    "rec = recall_score(valid_subset.label, y_pred,average='weighted',zero_division=0)\n",
    "f1 = f1_score(valid_subset.label, y_pred,average='weighted',zero_division=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict_subsets = {\n",
    "    'All symbols': {\n",
    "        'xgboost_winning': {\n",
    "            'accuracy': [],\n",
    "            'precision': [],\n",
    "            'recall': [],\n",
    "            'f1': []\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Store performance metrics in dictionary\n",
    "task = 'All symbols'\n",
    "model_name = 'xgboost_winning'\n",
    "metrics_dict_subsets['All symbols']['xgboost_winning'] = {'accuracy' : acc,\n",
    "                                 'precision' : prec,\n",
    "                                 'recall' : rec,\n",
    "                                 'f1' : f1,\n",
    "                                 }\n",
    "\n",
    "# Display performance metrics\n",
    "display_metrics(task, model_name, metrics_dict_subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classify digits vs. letters model showdown\n",
    "\n",
    "Perform a full showdown classifying digits vs letters:\n",
    "\n",
    "1. Create a column for whether each row is a digit or a letter\n",
    "2. Choose an evaluation metric \n",
    "3. Choose several candidate models to train\n",
    "4. Divide data to reserve a validation set that will NOT be used in training/testing\n",
    "5. K-fold train/test\n",
    "    1. Create train/test splits from the non-validation dataset \n",
    "    2. Train each candidate model (best practice: use the same split for all models)\n",
    "    3. Apply the model the the test split \n",
    "    4. (*Optional*) Perform hyper-parametric search\n",
    "    5. Record the model evaluation metrics\n",
    "    6. Repeat with a new train/test split\n",
    "6. Promote winner, apply model to validation set\n",
    "7. (*Optional*) Perform hyper-parametric search, if applicable\n",
    "8. Report model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a column for whether each row is a digit(=0) or a letter(=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the sample size to 10% of all data for the preliminary analysis\n",
    "train_subset = train.sample(n= 69000, replace=False, random_state=42)\n",
    "valid_subset = valid.sample(n= 11000, replace=False, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If use all data\n",
    "# train_subset = train\n",
    "# valid_subset = valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a column for whether each row is a digit(=0) or a letter(=1)\n",
    "def recategorize(label):\n",
    "    if str(label).isdigit() and 0 <= int(label) <= 9:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "train['recategory'] = train['label'].apply(recategorize)\n",
    "valid['recategory'] = valid['label'].apply(recategorize)\n",
    "train_subset['recategory'] = train_subset['label'].apply(recategorize)\n",
    "valid_subset['recategory'] = valid_subset['label'].apply(recategorize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Choose an evaluation metric: F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Choose several candidate models to train: Logistic Regression and XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Divide data to reserve a validation set that will NOT be used in training/testing: 'valid' is the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. K-fold train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. K-fold train/test\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state= 42)\n",
    "\n",
    "\n",
    "scores  = []\n",
    "test_label = train_subset['recategory']\n",
    "\n",
    "for train_index, test_index in cv.split(train_subset):\n",
    "    X_train2, X_test2 = train_subset.iloc[train_index], train_subset.iloc[test_index]\n",
    "    y_train2, y_test2 = test_label.iloc[train_index],test_label.iloc[test_index]\n",
    "    X_train2 = X_train2['image_flat'].tolist()\n",
    "    X_test2 = X_test2['image_flat'].tolist()\n",
    "\n",
    " # Initialize XGBoost classifier   \n",
    "    xgb_clf = XGBClassifier(n_estimators=100, random_state=42)\n",
    "    \n",
    "    # Train the model\n",
    "    xgb_clf.fit(X_train2, y_train2)\n",
    "    \n",
    "    # Evaluate the model using test data\n",
    "    y_pred = xgb_clf.predict(X_test2)\n",
    "    score = f1_score(y_test2, y_pred,average='macro')\n",
    "    scores.append(score)\n",
    "\n",
    "# Calculate average scores\n",
    "average_score = np.mean(scores)\n",
    "print(f\"Average score of XGBoost: {average_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=5, shuffle=True, random_state= 42)\n",
    "\n",
    "\n",
    "scores  = []\n",
    "test_label = train_subset['recategory']\n",
    "\n",
    "for train_index, test_index in cv.split(train_subset):\n",
    "    X_train2, X_test2 = train_subset.iloc[train_index], train_subset.iloc[test_index]\n",
    "    y_train2, y_test2 = test_label.iloc[train_index],test_label.iloc[test_index]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train2_scaled = scaler.fit_transform(X_train2['image_flat'].tolist())\n",
    "    X_test2_scaled = scaler.transform(X_test2['image_flat'].tolist())\n",
    "\n",
    "# Initialize logistic regression classifier\n",
    "    lr_clf = LogisticRegression(solver='saga', max_iter=1000, random_state=42)\n",
    "\n",
    "    # Train the model\n",
    "    lr_clf.fit(X_train2_scaled, y_train2)\n",
    "    \n",
    "    # Evaluate the model using test data\n",
    "    y_pred = lr_clf.predict(X_test2_scaled)\n",
    "    score = f1_score(y_test2, y_pred,average='macro')\n",
    "    scores.append(score)\n",
    "\n",
    "# Calculate the average score\n",
    "average_score = np.mean(scores)\n",
    "print(f\"Average score of Logistic: {average_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Improve the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'min_child_weight': [1, 2, 3],\n",
    "    'subsample': [0.8, 0.9, 1.0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator=xgb_clf, param_grid=param_grid, cv=3, scoring='f1', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_test2['image_flat'].tolist(), y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best Scores of XGBoost: {best_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buil the best model\n",
    "best_mdl_XGBoost = XGBClassifier(**grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': np.logspace(-4, 4, 20),\n",
    "    'penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(lr_clf, param_grid, cv=3, scoring='f1', n_jobs=-1)\n",
    "\n",
    "\n",
    "grid_search.fit(X_test2['image_flat'].tolist(), y_test2)\n",
    "\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best scores of Logistic:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the winning model\n",
    "best_mdl_Logistic = LogisticRegression(**grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance the winning model in the valid data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 score of the XGBoost is higher than the logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winning_mdl = best_mdl_XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winning_mdl.fit(X_test2['image_flat'].tolist(), y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = winning_mdl.predict(valid_subset['image_flat'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the scores of the best model\n",
    "acc = accuracy_score(valid_subset.recategory, y_pred)\n",
    "prec = precision_score(valid_subset.recategory, y_pred,average='weighted',zero_division=0)\n",
    "rec = recall_score(valid_subset.recategory, y_pred,average='weighted',zero_division=0)\n",
    "f1 = f1_score(valid_subset.recategory, y_pred,average='weighted',zero_division=0)\n",
    "\n",
    "cm4 = confusion_matrix(valid_subset.recategory, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict_subsets = {\n",
    "    'digit or letters': {\n",
    "        'xgboost_winning': {\n",
    "            'accuracy': [],\n",
    "            'precision': [],\n",
    "            'recall': [],\n",
    "            'f1': [] ,\n",
    "            'cm':[]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Store performance metrics in dictionary\n",
    "task = 'digit or letters'\n",
    "model_name = 'xgboost_winning'\n",
    "metrics_dict_subsets['digit or letters']['xgboost_winning'] = {'accuracy' : acc,\n",
    "                                 'precision' : prec,\n",
    "                                 'recall' : rec,\n",
    "                                 'f1' : f1,\n",
    "                                 'cm':cm4\n",
    "                                 }\n",
    "\n",
    "# Display performance metrics\n",
    "display_metrics_cm(task, model_name, metrics_dict_subsets)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
